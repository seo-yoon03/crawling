{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 크롬 드라이버 경로 설정\n",
    "chromedriver_path = os.path.join(os.getcwd(), \"chromedriver.exe\")  # 크롬 드라이버 경로를 입력해주세요.\n",
    "\n",
    "# 크롬 드라이버 서비스 객체 생성\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 웹 드라이버 객체 생성\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 크롤링할 URL\n",
    "url = 'https://www.boannews.com/media/t_list.asp?mkind=0'\n",
    "\n",
    "# 웹 페이지 열기\n",
    "driver.get(url)\n",
    "\n",
    "# 뉴스 데이터를 저장할 리스트\n",
    "news_data = []\n",
    "\n",
    "# 크롤링할 페이지 수\n",
    "num_pages = 2  # 원하는 페이지 수를 입력해주세요.\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    # 현재 페이지의 HTML 소스 코드 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 뉴스 목록 찾기\n",
    "    news_list = soup.select('.news_list a:not(.news_content)')\n",
    "\n",
    "    for news in news_list:\n",
    "        # 뉴스 링크 추출\n",
    "        news_url = 'https://www.boannews.com' + news['href']\n",
    "\n",
    "        # 새 탭에서 뉴스 링크 열기\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(news_url)\n",
    "\n",
    "        # 뉴스 내용 크롤링\n",
    "        news_html = driver.page_source\n",
    "        news_soup = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "        # 뉴스 제목\n",
    "        news_title = news_soup.select_one('#news_title02 h1').text.strip()\n",
    "\n",
    "        # 뉴스 날짜\n",
    "        news_date = ':'.join(news_soup.select_one('#news_util01').text.strip().split(':')[1:]).strip()\n",
    "\n",
    "        # 뉴스 내용\n",
    "        news_content = news_soup.select_one('#news_content').text.strip()\n",
    "\n",
    "        # 크롤링한 데이터를 리스트에 추가\n",
    "        news_data.append([news_title, news_date, news_content])\n",
    "\n",
    "        # 탭 닫기\n",
    "        driver.close()\n",
    "\n",
    "        # 원래 탭으로 돌아가기\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    # 다음 페이지로 이동\n",
    "    next_page_url = f'https://www.boannews.com/media/t_list.asp?Page={page+1}&kind=0'\n",
    "    driver.get(next_page_url)\n",
    "    time.sleep(2)  # 2초 대기 (페이지 로딩을 위해)\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# CSV 파일로 저장\n",
    "with open('news_data3.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Date', 'Content'])\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print('크롤링이 완료되었습니다.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
